{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Sistema de Recomendacion Predictivo de Productos__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Resumen y Contexto Comercial__:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Cervecería y Maltería Quilmes` (`CMQ`) tiene una aplicación B2B llamada `BEES`, que permite a empresas realizar pedidos de productos. Tras realizar encuestas de satisfacción del cliente, se identificó que uno de los puntos clave para mejorar la experiencia de usuario es la creación de un nuevo módulo que `ayude a los clientes a encontrar los productos que son más propensos a comprar en su próximo pedido`. Esto busca `reducir el tiempo dedicado a completar cada compra`, mejorando la experiencia general.\n",
    "\n",
    "El equipo de ciencia de datos de `CMQ` ha sido encargado de desarrollar un modelo de Machine Learning que prediga qué productos comprará cada cliente en su siguiente pedido. Este modelo será integrado en la aplicación `BEES`, generando recomendaciones personalizadas de productos. El equipo de producto utilizará el output del modelo para diseñar y desplegar el nuevo módulo dentro de la app, mientras que el equipo de ingeniería se encargará de la integración del modelo en la arquitectura de datos mediante pipelines.\n",
    "\n",
    "El objetivo principal es que el modelo prediga las compras a nivel individual de cada cliente de manera diaria, basado en patrones de compra anteriores y atributos asociados a cada cliente y producto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Características de los Datos__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Atributos**: Atributos asociados a cada cliente, proporcionando un perfil detallado de sus comportamientos y características.\n",
    "\n",
    "- **`Unnamed: 0`**: Índice generado automáticamente, no relevante para el análisis.\n",
    "\n",
    "- **`POC`**: Código de identificación único del cliente, equivalente a `ACCOUNT_ID` en la tabla de transacciones.\n",
    "\n",
    "- **`BussinessSegment`**: Clasificación del cliente según su nivel de uso de la aplicación BEES. Puede tomar los valores `PowerUsage`, `HighUsage`, `MediumUsage`, y `MinimalUsage`.\n",
    "\n",
    "- **`totalVolumen`**: Volumen total en hectolitros adquirido por el cliente durante el período de análisis (junio-agosto).\n",
    "\n",
    "- **`SkuDistintosPromediosXOrden`**: Promedio de productos distintos comprados por el cliente en cada orden durante el período de análisis.\n",
    "\n",
    "- **`SkuDistintosToTales`**: Total de productos distintos adquiridos por el cliente en el período.\n",
    "\n",
    "- **`concentracion`**: Estimación de la concentración de negocios diferentes a 150 metros del punto de venta. Puede tener los valores `Alto`, `Medio`, `Bajo`, o `S/D` (Sin Datos).\n",
    "\n",
    "- **`nse`**: Nivel socioeconómico de los habitantes cercanos al punto de venta. Puede ser `Alto`, `Medio`, `Bajo`, o `S/D` (Sin Datos).\n",
    "\n",
    "- **`segmentoUnico`**: Clasificación del cliente basada en su capacidad de compra. Puede tomar los valores `Inactivos`, `Masivos`, `Potenciales`, `Activos`.\n",
    "\n",
    "- **`canal`**: Canal de marketing al que pertenece el cliente, con valores como `COMIDA`, `Tradicional`, `BEBIDA`, `Mayorista`, `Kioscos/Maxikioscos`, entre otros.\n",
    "\n",
    "\n",
    "#### **Transacciones**: Contiene información de las transacciones realizadas por los clientes a través de la aplicación BEES de CMQ. Cada fila representa una compra específica realizada por un cliente.\n",
    "\n",
    "- **`Unnamed: 0`**: Índice generado automáticamente, no relevante para el análisis.\n",
    "\n",
    "- **`ACCOUNT_ID`**: Código de identificación único para cada cliente.\n",
    "\n",
    "- **`SKU_ID`**: Código de identificación único de cada producto vendido.\n",
    "\n",
    "- **`INVOICE_DATE`**: Fecha en la que se realizó la transacción, en formato numérico `yyyyMMdd`.\n",
    "\n",
    "- **`ORDER_ID`**: Código identificador del pedido.\n",
    "\n",
    "- **`ITEMS_PHYS_CASES`**: Número de bultos físicos comprados de un producto específico en la transacción.\n",
    "\n",
    "\n",
    "Esta descripción proporciona una visión clara de los datos disponibles en ambas tablas, lo que permitirá realizar análisis exploratorios y aplicar técnicas de machine learning para las recomendaciones de productos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Carga de Datos__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leer los archivos `atributos.csv` y `transactions.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from surprise import Dataset, Reader, SVD, accuracy\n",
    "from surprise.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos\n",
    "clients = pd.read_csv('atributos.csv')\n",
    "transactions = pd.read_csv('transacciones.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### __`Clientes`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar los primeros registros\n",
    "clients.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar la estructura de los datos\n",
    "clients.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información del DataFrame clientes\n",
    "clients.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que la base de datos `clientes` esta compuesta por 4400 registros y 10 columnas. Por otro lado, renombraremos las columnas para mayor comprensión, considerando buenas prácticas de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renombrar columnas\n",
    "new_cols = ['UNNAMED: 0', 'ACCOUNT_ID', 'BUSINESS_SEGMENT', 'TOTAL_VOLUME', 'DIFF_SKU_BY_ORDER',\n",
    "            'TOTAL_DIFF_SKU', 'CONCENTRATION', 'NSE', 'UNIQUE_SEGMENT', 'CHANNEL']\n",
    "\n",
    "clients.columns = new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de clientes únicos\n",
    "clients['ACCOUNT_ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos la existencia de duplicados o datos nulos\n",
    "clients_duplicated = clients.duplicated().sum()\n",
    "clients_null = clients.isnull().sum()\n",
    "\n",
    "print(f\"Clientes Duplicados: {clients_duplicated}\")\n",
    "print()\n",
    "print(f\"Clientes Nulos:\\n\\n{clients_null}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porcentaje de valores nulos por columna\n",
    "clients_null_percent = clients_null / len(clients) * 100\n",
    "print(f\"Porcentaje de valores nulos por columna:\\n\\n{clients_null_percent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que la columna `UNIQUE_SEGMENT` presenta 75 valores nulos y la columna `CHANNEL` 14. Por lo que investigaremos aquellas filas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clientes con valores nulos\n",
    "clients[clients['UNIQUE_SEGMENT'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteo de valores para la columna CHANNEL\n",
    "clients[clients['UNIQUE_SEGMENT'].isna()]['CHANNEL'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No hay filas duplicadas en el dataset `clientes`, lo cual es positivo ya que no será necesario realizar ningún tratamiento adicional para eliminarlos.\n",
    "\n",
    "Clientes con valores nulos:\n",
    "\n",
    "* `UNIQUE_SEGMENT`: Hay 75 registros con valores faltantes.\n",
    "\n",
    "* `CHANNEL`: Hay 14 registros con valores faltantes, pertenecientes a `UNIQUE_SEGMENT`.\n",
    "\n",
    "En la proxima sección tendremos que decidir si eliminar aquellos valores faltantes, imputarlos o crear una nueva característica llamada `Desconocido`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteo de valores para variables categóricas\n",
    "print(clients['BUSINESS_SEGMENT'].value_counts())\n",
    "print()\n",
    "print(clients['CONCENTRATION'].value_counts())\n",
    "print()\n",
    "print(clients['NSE'].value_counts())\n",
    "print()\n",
    "print(clients['UNIQUE_SEGMENT'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteo de valores para la columna CHANNEL\n",
    "clients['CHANNEL'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descripción estadística de clientes\n",
    "clients.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En resumen, hay una alta variabilidad en el comportamiento de los clientes en términos de volumen y variedad de productos comprados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### `Transacciones`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar los primero registros para transacciones\n",
    "transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cantidad de filas y columnas\n",
    "transactions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información del dataset\n",
    "transactions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar duplicados y valores nulos\n",
    "print(f\"Transacciones Duplicadas: {transactions.duplicated().sum()}\")\n",
    "print()\n",
    "print(f\"Transacciones Nulas:\\n\\n{transactions.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cantidad de ordenes, clientes, productos y fechas unicas\n",
    "print(f\"Ordenes unicas: {transactions['ORDER_ID'].nunique()}\")\n",
    "print(f\"Clientes unicos: {transactions['ACCOUNT_ID'].nunique()}\")\n",
    "print(f\"Productos unicos: {transactions['SKU_ID'].nunique()}\")\n",
    "print(f\"Fechas de compra unicas: {transactions['INVOICE_DATE'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Tamaño del Dataset`: El conjunto de datos tiene 280,828 filas y 6 columnas, lo que muestra un registro detallado de las transacciones de la compañía.\n",
    "\n",
    "`Información de Columnas`: No hay valores nulos ni duplicados en ninguna de las columnas. Las columnas contienen principalmente datos numéricos, excepto por ORDER_ID, que es de tipo object, lo que indica que se trata de un identificador alfanumérico.\n",
    "\n",
    "`Unicidad`:\n",
    "\n",
    "* __Ordenes únicas__: Hay 45,547 órdenes de compra distintas.\n",
    "\n",
    "* __Clientes únicos__: El conjunto de datos incluye 4,535 clientes diferentes.\n",
    "\n",
    "* __Productos únicos__: Se compraron 530 productos distintos.\n",
    "\n",
    "* __Fechas de compra únicas__: Las compras se distribuyeron a lo largo de 77 fechas diferentes, lo que indica que los datos cubren un período considerable de tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descripción estadística de las transacciones\n",
    "transactions.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mayoría de los clientes realiza compras pequeñas, con un promedio de casi 4 productos por transacción, aunque hay transacciones mucho mayores que llegan hasta 2,000 productos. Las fechas de las transacciones son consistentes en 2022, y existe una gran dispersión en los tipos de productos comprados. La distribución de clientes y productos sugiere que hay comportamientos de compra variados que podrían ser útiles para los modelos de recomendación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Transformacion de Datos__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### __`Clientes`__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La columna `UNNAMED: 0` no contiene información relevante por lo que la eliminaremos del Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar la columna 'UNNAMED: 0'\n",
    "clients.drop('UNNAMED: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evitar introducir sesgos en los datos, no realizaremos imputaciones ni crearemos una nueva característica que podría aumentar la cardinalidad del problema. Dado que los valores nulos en las columnas `UNIQUE_SEGMENT` y `CHANNEL` representan menos del 2% del total, eliminaremos esas filas para conservar la integridad de la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar las filas con valores nulos\n",
    "clients.dropna(inplace=True)\n",
    "\n",
    "# Verificar que no hay valores nulos\n",
    "clients.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### __`Transacciones`__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La columna `Unnamed: 0` no contiene información relevante por lo que la eliminaremos del Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar la columna 'Unnamed: 0'\n",
    "transactions.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La columna `INVOIDE_DATE` esta en formato `int64` por lo que la transformaremos a formato `datetime` para crear carácteristicas temporales que pueden ser útiles para el modelo de machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir INVOICE_DATE a datetime\n",
    "transactions['INVOICE_DATE'] = pd.to_datetime(transactions['INVOICE_DATE'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Modelo Relacional__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La unión de los dataframes `transactions` y `clients` es esencial para enriquecer el conjunto de datos transaccionales con información detallada de los clientes. Esto permite generar nuevas características valiosas para el análisis, como recencia, frecuencia y monto monetario (RFM), y entender mejor el comportamiento de compra. Al combinar ambas fuentes de datos, se puede realizar una segmentación más precisa de los clientes y mejorar la personalización de las recomendaciones. Además, la unión mediante un `left join` asegura que se mantengan todas las transacciones, incluso las de clientes nuevos, lo que facilita la identificación de brechas de información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unir los dataframes\n",
    "mdf = pd.merge(transactions, clients, on='ACCOUNT_ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar que no hay valores nulos\n",
    "mdf.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numero de clientes nuevos sin información\n",
    "mdf[mdf['CHANNEL'].isna()]['ACCOUNT_ID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al revisar los valores nulos en el dataframe combinado (`mdf`), observamos que algunas columnas relacionadas con los clientes, como `BUSINESS_SEGMENT`, `TOTAL_VOLUME`, `DIFF_SKU_BY_ORDER`, y otras, tienen 6165 valores nulos. Esto indica que hay clientes nuevos en las transacciones que no tienen información previa en el dataframe de clientes. Específicamente, 231 clientes son nuevos y no cuentan con datos adicionales en el dataframe de clientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que no tenemos datos suficientes sobre ellos, los excluiremos del modelo para evitar introducir ruido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar valores nulos\n",
    "clean_mdf = mdf.dropna(subset=['CHANNEL'])\n",
    "clean_mdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Feature Engineering__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### __`Características Temporales`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear columnas 'MES', 'DIA', 'DAY_OF_WEEK'\n",
    "clean_mdf['MES'] = clean_mdf['INVOICE_DATE'].dt.month\n",
    "clean_mdf['DIA'] = clean_mdf['INVOICE_DATE'].dt.day\n",
    "clean_mdf['DAY_OF_WEEK'] = clean_mdf['INVOICE_DATE'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear características de desfase\n",
    "clean_mdf['LAG_1'] = clean_mdf.groupby('ACCOUNT_ID')['ITEMS_PHYS_CASES'].shift(1)\n",
    "clean_mdf['LAG_2'] = clean_mdf.groupby('ACCOUNT_ID')['ITEMS_PHYS_CASES'].shift(2)\n",
    "clean_mdf['LAG_3'] = clean_mdf.groupby('ACCOUNT_ID')['ITEMS_PHYS_CASES'].shift(3)\n",
    "clean_mdf['LAG_4'] = clean_mdf.groupby('ACCOUNT_ID')['ITEMS_PHYS_CASES'].shift(4)\n",
    "clean_mdf['LAG_5'] = clean_mdf.groupby('ACCOUNT_ID')['ITEMS_PHYS_CASES'].shift(5)\n",
    "clean_mdf['LAG_6'] = clean_mdf.groupby('ACCOUNT_ID')['ITEMS_PHYS_CASES'].shift(6)\n",
    "clean_mdf['LAG_7'] = clean_mdf.groupby('ACCOUNT_ID')['ITEMS_PHYS_CASES'].shift(7)\n",
    "clean_mdf['LAG_8'] = clean_mdf.groupby('ACCOUNT_ID')['ITEMS_PHYS_CASES'].shift(8)\n",
    "clean_mdf['LAG_9'] = clean_mdf.groupby('ACCOUNT_ID')['ITEMS_PHYS_CASES'].shift(9)\n",
    "clean_mdf['LAG_10'] = clean_mdf.groupby('ACCOUNT_ID')['ITEMS_PHYS_CASES'].shift(10)\n",
    "clean_mdf['LAG_11'] = clean_mdf.groupby('ACCOUNT_ID')['ITEMS_PHYS_CASES'].shift(11)\n",
    "clean_mdf['LAG_12'] = clean_mdf.groupby('ACCOUNT_ID')['ITEMS_PHYS_CASES'].shift(12)\n",
    "clean_mdf['LAG_13'] = clean_mdf.groupby('ACCOUNT_ID')['ITEMS_PHYS_CASES'].shift(13)\n",
    "clean_mdf['LAG_14'] = clean_mdf.groupby('ACCOUNT_ID')['ITEMS_PHYS_CASES'].shift(14)\n",
    "\n",
    "# Crear medias moviles\n",
    "clean_mdf['MA_3'] = clean_mdf.groupby('ACCOUNT_ID')['ITEMS_PHYS_CASES'].transform(lambda x: x.rolling(3).mean())\n",
    "clean_mdf['MA_6'] = clean_mdf.groupby('ACCOUNT_ID')['ITEMS_PHYS_CASES'].transform(lambda x: x.rolling(6).mean())\n",
    "clean_mdf['MA_9'] = clean_mdf.groupby('ACCOUNT_ID')['ITEMS_PHYS_CASES'].transform(lambda x: x.rolling(9).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar que no hay valores nulos\n",
    "clean_mdf.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rellenar valores nulos\n",
    "clean_mdf.fillna(0, inplace=True)\n",
    "\n",
    "# Verificar que no hay valores nulos\n",
    "clean_mdf.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_mdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### __`Recencia, Frecuencia y Volumen`__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado el objetivo del proyecto, que es predecir qué productos comprará un cliente en su próximo pedido en la aplicación B2B BEES, el enfoque más adecuado es `calcular las métricas a nivel de cliente`. Debemos entender el comportamiento general de cada cliente para hacer recomendaciones personalizadas basadas en su historial de compras, en lugar de hacerlo a nivel de transacción.\n",
    "\n",
    "Razones por que el enfoque a nivel de cliente es más adecuado:\n",
    "\n",
    "* __`Recencia`__: A nivel de cliente, interesa saber cuánto tiempo ha pasado desde la última compra, ya que esto puede influir en la probabilidad de que realice otra compra pronto.\n",
    "\n",
    "* __`Frecuencia`__: Saber cuántas órdenes distintas ha realizado cada cliente. Esto es clave para identificar patrones de compra, como si un cliente compra con regularidad o esporádicamente.\n",
    "\n",
    "* __`Total de ítems comprados`__: A nivel de cliente, el volumen total de ítems comprados te permite identificar a clientes que hacen compras grandes o pequeñas, lo que es importante para determinar recomendaciones de productos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Calcular la fecha máxima de transacciones\n",
    "last_date = clean_mdf['INVOICE_DATE'].max()\n",
    "\n",
    "# Calcular recencia\n",
    "recency_df = clean_mdf.groupby('ACCOUNT_ID').agg(last_purchase=('INVOICE_DATE', 'max')).reset_index()\n",
    "recency_df['RECENCIA'] = (last_date - recency_df['last_purchase']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la frecuencia\n",
    "frequency_df = clean_mdf.groupby('ACCOUNT_ID')['ORDER_ID'].nunique().reset_index()\n",
    "frequency_df.columns = ['ACCOUNT_ID', 'FRECUENCIA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el Volumen\n",
    "monetary_df = clean_mdf.groupby('ACCOUNT_ID')['ITEMS_PHYS_CASES'].sum().reset_index()\n",
    "monetary_df.columns = ['ACCOUNT_ID', 'TOTAL_ITEMS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_df = pd.merge(recency_df[['ACCOUNT_ID', 'RECENCIA']],\n",
    "                  frequency_df[['ACCOUNT_ID', 'FRECUENCIA']],\n",
    "                  on='ACCOUNT_ID')\n",
    "rfm_df = pd.merge(rfm_df, monetary_df[['ACCOUNT_ID', 'TOTAL_ITEMS']], on='ACCOUNT_ID')\n",
    "\n",
    "rfm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusionar las tablas\n",
    "df_merged = pd.merge(rfm_df, clean_mdf, how='inner', on='ACCOUNT_ID')\n",
    "\n",
    "# Verificar el DataFrame resultante\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar las columnas\n",
    "df_merged.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Análisis Exploratorio de Datos (EDA)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables Numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el tamaño de la figura para los gráficos\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Crear el primer gráfico: Distribución de 'ITEMS_PHYS_CASES'\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.histplot(df_merged['ITEMS_PHYS_CASES'], bins=50, kde=True, color='blue')\n",
    "plt.title('Distribución de ITEMS_PHYS_CASES')\n",
    "plt.xlabel('ITEMS_PHYS_CASES')\n",
    "plt.ylabel('Frecuencia')\n",
    "\n",
    "# Crear el segundo gráfico: Distribución de 'TOTAL_VOLUME'\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.histplot(df_merged['TOTAL_VOLUME'], bins=50, kde=True, color='green')\n",
    "plt.title('Distribución de TOTAL_VOLUME')\n",
    "plt.xlabel('TOTAL_VOLUME')\n",
    "plt.ylabel('Frecuencia')\n",
    "\n",
    "# Crear el tercer gráfico: Distribución de 'DIFF_SKU_BY_ORDER'\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(df_merged['DIFF_SKU_BY_ORDER'], bins=50, kde=True, color='red')\n",
    "plt.title('Distribución de DIFF_SKU_BY_ORDER')\n",
    "plt.xlabel('DIFF_SKU_BY_ORDER')\n",
    "plt.ylabel('Frecuencia')\n",
    "\n",
    "# Crear el cuarto gráfico: Distribución de 'TOTAL_DIFF_SKU'\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.histplot(df_merged['TOTAL_DIFF_SKU'], bins=50, kde=True, color='purple')\n",
    "plt.title('Distribución de TOTAL_DIFF_SKU')\n",
    "plt.xlabel('TOTAL_DIFF_SKU')\n",
    "plt.ylabel('Frecuencia')\n",
    "\n",
    "# Ajustar el espacio entre los gráficos\n",
    "plt.tight_layout()\n",
    "\n",
    "# Mostrar los gráficos\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una figura con 4 subplots (2 filas, 2 columnas)\n",
    "fig = make_subplots(rows=2, cols=2, subplot_titles=('Distribución de ITEMS_PHYS_CASES',\n",
    "                                                    'Distribución de TOTAL_VOLUME',\n",
    "                                                    'Distribución de DIFF_SKU_BY_ORDER',\n",
    "                                                    'Distribución de TOTAL_DIFF_SKU'))\n",
    "\n",
    "# Crear boxplot para ITEMS_PHYS_CASES\n",
    "fig.add_trace(go.Box(y=df_merged['ITEMS_PHYS_CASES'], name='ITEMS_PHYS_CASES', marker_color='blue'), row=1, col=1)\n",
    "\n",
    "# Crear boxplot para TOTAL_VOLUME\n",
    "fig.add_trace(go.Box(y=df_merged['TOTAL_VOLUME'], name='TOTAL_VOLUME', marker_color='green'), row=1, col=2)\n",
    "\n",
    "# Crear boxplot para DIFF_SKU_BY_ORDER\n",
    "fig.add_trace(go.Box(y=df_merged['DIFF_SKU_BY_ORDER'], name='DIFF_SKU_BY_ORDER', marker_color='red'), row=2, col=1)\n",
    "\n",
    "# Crear boxplot para TOTAL_DIFF_SKU\n",
    "fig.add_trace(go.Box(y=df_merged['TOTAL_DIFF_SKU'], name='TOTAL_DIFF_SKU', marker_color='purple'), row=2, col=2)\n",
    "\n",
    "# Actualizar el layout de la figura\n",
    "fig.update_layout(height=600, width=800, title_text=\"Boxplots de Variables Seleccionadas\", showlegend=False)\n",
    "\n",
    "# Mostrar la figura\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __ITEMS_PHYS_CASES__: La mayoría de los valores se encuentran en rangos bajos, pero existen algunos valores atípicos significativos, que llegan hasta 2000. Esto indica que algunos clientes compran cantidades significativamente mayores de productos en comparación con el promedio.\n",
    "\n",
    "* __TOTAL_VOLUME__: Al igual que en el caso anterior, la mayoría de los valores se concentran en un rango pequeño, con algunos outliers que superan los 4000. Esto también sugiere que hay casos de volúmenes de compra mucho más altos que los típicos.\n",
    "\n",
    "* __DIFF_SKU_BY_ORDER__: La dispersión de esta variable es mucho menor. Aunque se observan algunos valores atípicos, la mayoría de los clientes compran entre 5 y 10 SKU diferentes por orden. Los valores atípicos no son tan extremos como en las variables anteriores.\n",
    "\n",
    "* __TOTAL_DIFF_SKU__: La mayoría de los clientes tienen entre 27 y 64 SKU distintos, con algunos outliers que alcanzan los 150. Esto indica que ciertos clientes compran una variedad significativamente mayor de productos en comparación con el resto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 productos más comprados\n",
    "top_skus = df_merged['SKU_ID'].value_counts().head(10)\n",
    "plt.figure(figsize=(10, 5))\n",
    "top_skus.plot(kind='bar')\n",
    "plt.title('Top 10 Productos (SKU) Más Comprados')\n",
    "plt.xlabel('SKU ID')\n",
    "plt.ylabel('Número de Compras')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que el producto con el SKU `7038` es el más popular, seguido de otros como el `19088`, `7651`, y `24880`, todos con un número significativo de compras. Esto indica una clara preferencia por ciertos productos, lo que puede ser clave para un sistema de recomendaciones, especialmente aquellos con comportamientos de compra similares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables Categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el tamaño de la figura para los gráficos\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Gráfico 1: Compras por Concentración de negocios (CONCENTRATION)\n",
    "plt.subplot(2, 2, 1)\n",
    "compras_por_concentracion = df_merged['CONCENTRATION'].value_counts()\n",
    "compras_por_concentracion.plot(kind='bar', color='blue')\n",
    "plt.title('Distribución de Compras por Concentración')\n",
    "plt.xlabel('Concentración')\n",
    "plt.ylabel('Número de Compras')\n",
    "\n",
    "# Gráfico 2: Compras por Nivel Socioeconómico (NSE)\n",
    "plt.subplot(2, 2, 2)\n",
    "compras_por_nse = df_merged['NSE'].value_counts()\n",
    "compras_por_nse.plot(kind='bar', color='green')\n",
    "plt.title('Distribución de Compras por NSE')\n",
    "plt.xlabel('Nivel Socioeconómico')\n",
    "plt.ylabel('Número de Compras')\n",
    "\n",
    "# Gráfico 3: Compras por Segmento Único (UNIQUE_SEGMENT)\n",
    "plt.subplot(2, 2, 3)\n",
    "compras_por_segmento = df_merged['UNIQUE_SEGMENT'].value_counts()\n",
    "compras_por_segmento.plot(kind='bar', color='red')\n",
    "plt.title('Distribución de Compras por Segmento Único')\n",
    "plt.xlabel('Segmento Único')\n",
    "plt.ylabel('Número de Compras')\n",
    "\n",
    "# Gráfico 4: Compras por Canal (CHANNEL)\n",
    "plt.subplot(2, 2, 4)\n",
    "compras_por_canal = df_merged['CHANNEL'].value_counts()\n",
    "compras_por_canal.plot(kind='bar', color='purple')\n",
    "plt.title('Distribución de Compras por Canal')\n",
    "plt.xlabel('Canal')\n",
    "plt.ylabel('Número de Compras')\n",
    "\n",
    "# Ajustar el espacio entre los gráficos\n",
    "plt.tight_layout()\n",
    "\n",
    "# Mostrar los gráficos\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mayoría de los clientes pertenecen a los segmentos `HighUsage` y `MediumUsage` de productos, lo que refleja un comportamiento de consumo considerable. En cuanto a la concentración, los niveles altos y medios predominan entre los clientes.\n",
    "\n",
    "El nivel socioeconómico `Medio` es el más común, con pocos clientes de nivel alto, lo que puede indicar una oportunidad para segmentar y personalizar ofertas para diferentes grupos.\n",
    "\n",
    "El segmento `Activos` domina en términos de actividad, aunque hay un número considerable de `Inactivos` y otros segmentos que podrían ser objeto de campañas de reactivación o retención.\n",
    "\n",
    "La mayoría de las transacciones provienen del canal `Tradicional`, lo que sugiere que este es el canal principal para la distribución de productos. Los `Kioscos/Maxikioscos` también representan una parte significativa del total, lo que indica que son un punto importante de venta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribución de la Recencia, Frecuencia y Total de Ítems Comprados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar número de bins para recencia\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(df_merged['RECENCIA'], bins=30, kde=True)\n",
    "plt.title('Distribución de la Recencia (días desde la última compra)')\n",
    "plt.xlabel('Recencia (días)')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()\n",
    "\n",
    "# Ajustar número de bins para frecuencia\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(df_merged['FRECUENCIA'], bins=30, kde=True)\n",
    "plt.title('Distribución de la Frecuencia (número de órdenes)')\n",
    "plt.xlabel('Frecuencia (órdenes)')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()\n",
    "\n",
    "# Ajustar número de bins para total_items\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(df_merged['TOTAL_ITEMS'], bins=50, kde=True)\n",
    "plt.title('Distribución del Total de Ítems Comprados')\n",
    "plt.xlabel('Total de Ítems Comprados')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Recencia`: La mayoría de los clientes son compradores recientes, lo cual es positivo para CMQ ya que sugiere que los clientes están activos en la plataforma BEES.\n",
    "\n",
    "* `Frecuencia`: La mayoría de los clientes compra con una frecuencia moderada. Pocos clientes son altamente frecuentes (más de 40 órdenes), lo que podría ser un grupo interesante para análisis mas profundos.\n",
    "\n",
    "* `Total de Ítems Comprados`: Hay una concentración de clientes que compran cantidades pequeñas, pero también algunos clientes grandes que compran cantidades significativas de productos, lo cual sugiere la presencia de clientes mayoristas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear subplots: 1 fila, 3 columnas\n",
    "fig = make_subplots(rows=1, cols=3, subplot_titles=(\"Recencia\", \"Frecuencia\", \"Total de Ítems Comprados\"))\n",
    "\n",
    "# Boxplot para Recencia\n",
    "fig.add_trace(go.Box(y=df_merged['RECENCIA'], name=\"Recencia\"), row=1, col=1)\n",
    "\n",
    "# Boxplot para Frecuencia\n",
    "fig.add_trace(go.Box(y=df_merged['FRECUENCIA'], name=\"Frecuencia\"), row=1, col=2)\n",
    "\n",
    "# Boxplot para Total de Ítems Comprados\n",
    "fig.add_trace(go.Box(y=df_merged['TOTAL_ITEMS'], name=\"Total Ítems\"), row=1, col=3)\n",
    "\n",
    "# Actualizar el layout\n",
    "fig.update_layout(title_text=\"Distribuciones de Recencia, Frecuencia y Total de Ítems Comprados\", showlegend=False)\n",
    "\n",
    "# Mostrar la figura\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Clientes recientes`: La gran mayoría de los clientes son compradores recientes, la mediana indica que en general cada 2 días los clientes hacen compras.\n",
    "\n",
    "* `Frecuencia`: Aunque la mediana de frecuencia es de 14 órdenes, hay algunos clientes que realizan muchas más compras (outliers).\n",
    "\n",
    "* `Volumen total de ítems`: La mayoría de los clientes hacen pedidos pequeños con una media de 206, mientras que hay unos pocos clientes que hacen pedidos muy grandes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patrones Temporales (MES, DIA, DAY_OF_WEEK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar los datos por mes para contar las órdenes\n",
    "orders_by_month = df_merged.groupby('MES')['ORDER_ID'].nunique().reset_index()\n",
    "\n",
    "# Gráfico de tendencia del número de órdenes por mes\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x='MES', y='ORDER_ID', data=orders_by_month, marker='o')\n",
    "plt.title('Número de Órdenes por Mes')\n",
    "plt.xlabel('Mes')\n",
    "plt.ylabel('Órdenes')\n",
    "plt.xticks(ticks=orders_by_month['MES'], labels=['Mayo', 'Junio', 'Julio', 'Agosto'])\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Compras por día de la semana\n",
    "orders_by_day = df_merged.groupby('DAY_OF_WEEK')['ORDER_ID'].nunique().reset_index()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x='DAY_OF_WEEK', y='ORDER_ID', data=orders_by_day, marker='o')\n",
    "plt.title('Órdenes por Día de la Semana')\n",
    "plt.xlabel('Día de la Semana')\n",
    "plt.ylabel('Órdenes')\n",
    "plt.xticks(ticks=orders_by_day['DAY_OF_WEEK'], labels=['Lunes', 'Martes', 'Miercoles', 'Jueves', 'Viernes', 'Sabado'])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Crecimiento Sostenido__: \n",
    "\n",
    "- El crecimiento mensual en las órdenes indica una tendencia positiva, y sugiere que se pueden realizar más esfuerzos para continuar promoviendo el uso de la plataforma BEES en los próximos meses.\n",
    "\n",
    "* __Patrones Semanales__:\n",
    "\n",
    "- La mayor parte de las compras se concentran los miercoles y viernes, lo que puede indicar que las empresas planifican sus compras durante estos días para tener stock a lo largo de la semana.\n",
    "\n",
    "- Los días jueves y sábado tienen una caída en la actividad, lo que sugiere que los esfuerzos logísticos y de marketing pueden ajustarse para concentrarse en los días de mayor actividad.\n",
    "\n",
    "- Lunes tiene pocas órdenes, lo que puede indicar que las empresas suelen empezar a planificar sus compras más adelante en la semana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar los datos por día para contar las órdenes\n",
    "orders_by_day_2 = df_merged.groupby('INVOICE_DATE')['ORDER_ID'].nunique().reset_index()\n",
    "\n",
    "# Gráfico de tendencia del número de órdenes por día\n",
    "fig = px.line(orders_by_day_2, x='INVOICE_DATE', y='ORDER_ID', title='Número de Órdenes por Día',\n",
    "              labels={'INVOICE_DATE': 'Fecha', 'ORDER_ID': 'Órdenes'}, markers=True)\n",
    "\n",
    "# Ajustar el formato de la fecha en el eje X para mejor visualización\n",
    "fig.update_xaxes(tickformat='%Y-%m-%d', tickangle=45)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Patrón Semanal`: Se observa un patrón cíclico que se repite aproximadamente cada semana, con un aumento en las órdenes seguido de una caída generalmente los Lunes. Esto podría deberse a la naturaleza del negocio, donde ciertos días de la semana tienen más actividad (los miercoles durante la semana y viernes al terminar la semana).\n",
    "\n",
    "* `Picos de Actividad`: Hay picos pronunciados de órdenes, lo que sugiere que hay días en los que los clientes tienden a realizar más compras, lo que podría estar relacionado con promociones, hábitos de compra o fechas específicas de alta demanda.\n",
    "\n",
    "* `Tendencias Estacionales`: No parece haber una tendencia estacional fuerte (incremento o decremento constante), sino que los picos y caídas se mantienen relativamente consistentes a lo largo del tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar los datos por día para contar las órdenes\n",
    "orders_by_day = df_merged.groupby('INVOICE_DATE')['ORDER_ID'].nunique().reset_index()\n",
    "\n",
    "# Agrupar las órdenes por fecha\n",
    "orders_by_day = df_merged.groupby('INVOICE_DATE').size().reset_index(name='orders_per_day')\n",
    "\n",
    "# Crear características de desfase para el número de órdenes del día anterior (lag 1)\n",
    "orders_by_day['lag_1'] = orders_by_day['orders_per_day'].shift(1)\n",
    "\n",
    "# Crear características de desfase para días anteriores hasta completar 2 semanas\n",
    "orders_by_day['lag_2'] = orders_by_day['orders_per_day'].shift(2)\n",
    "orders_by_day['lag_3'] = orders_by_day['orders_per_day'].shift(3)\n",
    "orders_by_day['lag_4'] = orders_by_day['orders_per_day'].shift(4)\n",
    "orders_by_day['lag_5'] = orders_by_day['orders_per_day'].shift(5)\n",
    "orders_by_day['lag_6'] = orders_by_day['orders_per_day'].shift(6)\n",
    "orders_by_day['lag_7'] = orders_by_day['orders_per_day'].shift(7)\n",
    "orders_by_day['lag_8'] = orders_by_day['orders_per_day'].shift(8)\n",
    "orders_by_day['lag_9'] = orders_by_day['orders_per_day'].shift(9)\n",
    "orders_by_day['lag_10'] = orders_by_day['orders_per_day'].shift(10)\n",
    "orders_by_day['lag_11'] = orders_by_day['orders_per_day'].shift(11)\n",
    "orders_by_day['lag_12'] = orders_by_day['orders_per_day'].shift(12)\n",
    "orders_by_day['lag_13'] = orders_by_day['orders_per_day'].shift(13)\n",
    "orders_by_day['lag_14'] = orders_by_day['orders_per_day'].shift(14)\n",
    "\n",
    "# Crear características de media móvil para 2,3,4,5,6,7 días\n",
    "orders_by_day['ma_3'] = orders_by_day['orders_per_day'].rolling(window=3).mean()\n",
    "orders_by_day['ma_6'] = orders_by_day['orders_per_day'].rolling(window=6).mean()\n",
    "orders_by_day['ma_9'] = orders_by_day['orders_per_day'].rolling(window=9).mean()\n",
    "orders_by_day['ma_14'] = orders_by_day['orders_per_day'].rolling(window=14).mean()\n",
    "\n",
    "# Mostrar el resultado\n",
    "orders_by_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la figura\n",
    "fig = go.Figure()\n",
    "\n",
    "# Asegúrate de que el DataFrame orders_by_day tiene las columnas 'INVOICE_DATE', 'orders_per_day', y 'ma_6'\n",
    "\n",
    "# Agregar la serie de tiempo original (órdenes por día)\n",
    "fig.add_trace(go.Scatter(x=orders_by_day['INVOICE_DATE'], y=orders_by_day['orders_per_day'],\n",
    "                         mode='lines', name='Órdenes por Día'))\n",
    "\n",
    "# Agregar la media móvil de 6 días\n",
    "fig.add_trace(go.Scatter(x=orders_by_day['INVOICE_DATE'], y=orders_by_day['ma_6'],\n",
    "                         mode='lines', name='Media Móvil 6 días', line=dict(color='green')))\n",
    "\n",
    "# Títulos y ajustes\n",
    "fig.update_layout(title='Tendencia de Órdenes por Día con Medias Móviles',\n",
    "                  xaxis_title='Fecha',\n",
    "                  yaxis_title='Número de Órdenes',\n",
    "                  template='plotly_white')\n",
    "\n",
    "# Mostrar la gráfica\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las fluctuaciones diarias son significativas, pero la media móvil muestra que, en general, el comportamiento de las órdenes es estable y con tendencia al alza. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establecer INVOICE_DATE como índice\n",
    "orders_by_day.set_index('INVOICE_DATE', inplace=True)\n",
    "\n",
    "# Eliminar valores nulos\n",
    "orders_by_day.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Descomponer la serie temporal remuestreada (usar la columna específica)\n",
    "decomposed = seasonal_decompose(orders_by_day['orders_per_day'], model='additive', period=7)\n",
    "\n",
    "# Filtrar el componente estacional para el rango de fechas deseado (21 de julio de 2022 al 6 de agosto de 2022)\n",
    "seasonal_filtered = decomposed.seasonal['2022-07-04':'2022-07-21']\n",
    "\n",
    "# Crear el gráfico en Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Añadir la línea del componente estacional\n",
    "fig.add_trace(go.Scatter(x=seasonal_filtered.index, y=seasonal_filtered, mode='lines', name='Componente Estacional'))\n",
    "\n",
    "# Títulos y etiquetas\n",
    "fig.update_layout(\n",
    "    title='Componente Estacional - 4 al 21 de Julio de 2022',\n",
    "    xaxis_title='Fecha',\n",
    "    yaxis_title='Número de Órdenes',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "# Mostrar el gráfico\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El gráfico del componente estacional revela un patrón cíclico claro en las órdenes del 4 al 21 de Julio de 2022, con picos y valles recurrentes. Estos ciclos sugieren que ciertos días de la semana tienen mayor demanda, posiblemente influenciados por eventos específicos o comportamientos de compra regulares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Data Wrangling y Modelado__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Preprocesamiento de Datos__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El escalado de las variables numéricas como `TOTAL_VOLUME`, `DIFF_SKU_BY_ORDER`, `TOTAL_DIFF_SKU`, `RECENCIA`, `FRECUENCIA` y `MONETARY` es crucial para garantizar que los modelos de aprendizaje automático, como `SVD` y `KNN`, funcionen correctamente. Estas variables pueden tener diferentes rangos de valores y magnitudes, lo que podría afectar negativamente el rendimiento del modelo, especialmente en algoritmos basados en distancias como KNN. Al aplicar un escalado estándar, transformamos estas variables para que tengan una media de 0 y una desviación estándar de 1, lo que asegura que todas las características numéricas tengan la misma importancia y escala en el proceso de entrenamiento, evitando que las variables con valores grandes dominen el modelo. Esto también mejora la estabilidad y precisión de las predicciones en los modelos supervisados y de filtrado colaborativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si las columnas existen en df_merged antes de aplicar el escalado\n",
    "numeric_cols = ['TOTAL_VOLUME', 'DIFF_SKU_BY_ORDER', 'TOTAL_DIFF_SKU', 'RECENCIA', 'FRECUENCIA', 'MONETARY'] \n",
    "numeric_cols_present = [col for col in numeric_cols if col in df_merged.columns]\n",
    "\n",
    "# Si las columnas están presentes, proceder con el escalado\n",
    "if numeric_cols_present:\n",
    "    standard_scaler = StandardScaler()\n",
    "    df_merged[numeric_cols_present] = standard_scaler.fit_transform(df_merged[numeric_cols_present])\n",
    "else:\n",
    "    print(f\"Las columnas {numeric_cols} no están presentes en el DataFrame.\")\n",
    "\n",
    "# Mostrar las primeras filas para verificar los resultados\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a normalizar las variables categoricas utilizando `OneHotEncoder` ya que crea una representación binaria adecuada para capturar la naturaleza de las variables categóricas sin asumir un orden implícito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar One-Hot Encoding en las columnas categóricas\n",
    "df_merged = pd.get_dummies(df_merged, columns=['BUSINESS_SEGMENT', 'CONCENTRATION', 'NSE', 'UNIQUE_SEGMENT', 'CHANNEL'], drop_first=True)\n",
    "\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos las columnas que no son relevantes para los modelos basados en sistemas de recomendación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar las columnas 'ORDER_ID' e 'INVOICE_DATE'\n",
    "df_merged = df_merged.drop(columns=['ORDER_ID', 'INVOICE_DATE'])\n",
    "\n",
    "# Verificar que se han eliminado correctamente\n",
    "print(df_merged.columns) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haremos una division del DataFrame en los meses de ``junio``, ``julio`` y ``agosto``, por la necesidad de evaluar el comportamiento de las predicciones de los modelos en periodos de tiempo específicos. Al segmentar los datos, podemos obtener información detallada sobre patrones de compra y variaciones en cada mes, lo cual es crucial en un sistema de recomendación. Estas son las razones principales para dividir el DataFrame por meses:\n",
    "\n",
    "``Evaluación del modelo en distintos periodos``: Al dividir los datos, podemos analizar si los modelos, como ``KNN`` o ``SVD``, funcionan de manera consistente a lo largo del tiempo o si existen diferencias significativas en su rendimiento en distintos meses.\n",
    "\n",
    "``Identificación de patrones estacionales o cambios de comportamiento``: Algunos productos pueden tener mayor demanda en un mes específico debido a factores estacionales, promociones o eventos. Dividir los datos por meses nos permite captar estos patrones que podrían mejorar las recomendaciones.\n",
    "\n",
    "``Validación temporal del modelo``: Al trabajar con datos históricos de compras, es útil dividir los datos cronológicamente para entrenar el modelo con un periodo (por ejemplo, junio-julio) y validarlo o testearlo con otro (agosto). Esto simula el comportamiento real del sistema en producción, donde las recomendaciones se generan basadas en el historial previo.\n",
    "\n",
    "``Predicciones más precisas``: Dividir los datos en diferentes meses permite ajustar mejor las predicciones a las condiciones del mercado o del cliente en un tiempo específico, lo que puede mejorar la personalización de las recomendaciones para cada cliente en función de los productos que fueron relevantes en ese mes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el DataFrame en conjuntos de datos para cada mes\n",
    "june_df = df_merged[df_merged['MES'] == 6] \n",
    "july_df = df_merged[df_merged['MES'] == 7]\n",
    "august_df = df_merged[df_merged['MES'] == 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar las dimensiones de los conjuntos de datos\n",
    "print(f\"Datos de junio: {june_df.shape}\")\n",
    "print(f\"Datos de julio: {july_df.shape}\")\n",
    "print(f\"Datos de agosto: {august_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a identificar clientes que realizaron compras en `julio` y `agosto`, lo que permite analizar su comportamiento recurrente y mejorar las predicciones de compra.\n",
    "\n",
    "Al centrarse en estos clientes, se pueden comparar sus compras entre ambos meses para detectar cambios en preferencias o la efectividad de promociones.\n",
    "\n",
    "Este enfoque mejora las recomendaciones personalizadas, basándose en patrones consistentes de compra.\n",
    "\n",
    "Además, facilita la evaluación del modelo, ya que permite entrenarlo con datos de julio y validarlo con las compras de agosto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los clientes que compraron tanto en julio como agosto\n",
    "clientes_julio = july_df['ACCOUNT_ID'].unique()\n",
    "clientes_agosto = august_df['ACCOUNT_ID'].unique()\n",
    "\n",
    "# Obtener la intersección de ambos conjuntos de clientes\n",
    "clientes_ambos_meses = list(set(clientes_julio).intersection(set(clientes_agosto)))\n",
    "\n",
    "# Filtrar los DataFrames para considerar solo esos clientes\n",
    "july_df_filtrado = july_df[july_df['ACCOUNT_ID'].isin(clientes_ambos_meses)]\n",
    "august_df_filtrado = august_df[august_df['ACCOUNT_ID'].isin(clientes_ambos_meses)]\n",
    "\n",
    "print(f\"Número de clientes en ambos meses: {len(clientes_ambos_meses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Desarrollo de Modelos para Sistemas de Recomendación__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para decidir si aplicar ``SVD`` (Singular Value Decomposition) o ``KNN`` (K-Nearest Neighbors) primero, es útil entender cómo cada enfoque puede resolver el problema del sistema de recomendación. Ambos enfoques son populares, pero tienen diferentes fortalezas:\n",
    "\n",
    "1. __``SVD (Singular Value Decomposition)``__:\n",
    "\n",
    "\n",
    "__Ventajas__:\n",
    "\n",
    "\n",
    "* __Recomendación colaborativa__: Es un enfoque basado en la descomposición de matrices, ideal para sistemas de recomendación colaborativa. Utiliza patrones en las compras de clientes y productos para hacer predicciones sobre qué productos comprarán los clientes en el futuro.\n",
    "\n",
    "* __Escalabilidad__: SVD es más eficiente y puede manejar grandes datasets, lo que lo hace ideal si tienes muchos productos (SKU) y clientes.\n",
    "\n",
    "* __Predicción precisa__: Es muy bueno para capturar relaciones latentes entre productos y clientes.\n",
    "\n",
    "* __Aplicabilidad__: Recomendable si tienes datos dispersos, es decir, muchos clientes compran solo algunos productos, pero no todos. En este caso, SVD puede aprovechar bien esos patrones.\n",
    "\n",
    "\n",
    "2. __``KNN (K-Nearest Neighbors)``__:\n",
    "\n",
    "\n",
    "__Ventajas__:\n",
    "\n",
    "\n",
    "* __Simplicidad__: KNN es fácil de implementar y puede ser eficaz para problemas con datos no tan grandes o donde la relación entre clientes y productos es más directa.\n",
    "\n",
    "* __Recomendación basada en similitud__: KNN puede recomendar productos basados en la similitud entre clientes o productos, comparando comportamientos de compra similares.\n",
    "\n",
    "* __Recomendación basada en proximidad__: Recomendaciones directas basadas en productos similares a los que ya compró el cliente.\n",
    "\n",
    "* __Aplicabilidad__: Recomendable si tienes un dataset moderado o si buscas identificar productos similares basados en patrones de compra de los clientes más cercanos (vecinos).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset, Reader, SVD, accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "# Cargar datos para el modelo\n",
    "reader = Reader(rating_scale=(1, 5))  # Ajustar según la escala de tu conjunto de datos\n",
    "data = Dataset.load_from_df(july_df_filtrado[['ACCOUNT_ID', 'SKU_ID', 'ITEMS_PHYS_CASES']], reader)\n",
    "\n",
    "# Dividir en conjunto de entrenamiento y prueba\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Entrenar modelo SVD\n",
    "modelo_svd = SVD()\n",
    "modelo_svd.fit(trainset)\n",
    "\n",
    "# Evaluar el rendimiento en el conjunto de prueba\n",
    "predictions = modelo_svd.test(testset)\n",
    "rmse = accuracy.rmse(predictions)\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNBasic\n",
    "\n",
    "# Opciones de similitud para KNN user-based\n",
    "sim_options_user = {\n",
    "    'name': 'cosine',        # Puede ser 'cosine' o 'pearson'\n",
    "    'user_based': True       # True para user-based\n",
    "}\n",
    "\n",
    "# Opciones de similitud para KNN item-based\n",
    "sim_options_item = {\n",
    "    'name': 'cosine',        # Puede ser 'cosine' o 'pearson'\n",
    "    'user_based': False      # False para item-based\n",
    "}\n",
    "\n",
    "# Modelo KNN para user-based\n",
    "modelo_knn_user = KNNBasic(sim_options=sim_options_user)\n",
    "modelo_knn_user.fit(trainset)\n",
    "\n",
    "# Hacer predicciones y calcular RMSE para KNN user-based\n",
    "predicciones_knn_user = modelo_knn_user.test(testset)\n",
    "rmse_knn_user = accuracy.rmse(predicciones_knn_user)\n",
    "print(f\"RMSE del modelo KNN (user-based): {rmse_knn_user}\")\n",
    "\n",
    "# Modelo KNN para item-based\n",
    "modelo_knn_item = KNNBasic(sim_options=sim_options_item)\n",
    "modelo_knn_item.fit(trainset)\n",
    "\n",
    "# Hacer predicciones y calcular RMSE para KNN item-based\n",
    "predicciones_knn_item = modelo_knn_item.test(testset)\n",
    "rmse_knn_item = accuracy.rmse(predicciones_knn_item)\n",
    "print(f\"RMSE del modelo KNN (item-based): {rmse_knn_item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Modelo K-Nearest Neighbors (KNN)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo ``K-Nearest Neighbors (KNN)`` es utilizado para generar recomendaciones basadas en la similitud entre productos (item-based) y entre usuarios (user-based). El flujo de trabajo incluye los siguientes pasos:\n",
    "\n",
    "1. __Preparación de los datos__: Los datos de julio se convierten al formato necesario para el uso en el modelo KNN utilizando la biblioteca surprise. Esto incluye definir las columnas clave como ``ACCOUNT_ID``, ``SKU_ID`` y ``ITEMS_PHYS_CASES``.\n",
    "\n",
    "2. __Entrenamiento de los modelos KNN__: Se entrenan dos modelos, uno basado en la similitud de productos y otro basado en la similitud de usuarios. La métrica utilizada es el coseno (``cosine similarity``), que es adecuada para medir la similitud en datos dispersos como estos.\n",
    "\n",
    "3. __Generación de predicciones__: Una vez entrenados los modelos, se generan predicciones con el modelo ``KNN (item-based)`` para evaluar su rendimiento en un conjunto de datos de prueba de agosto.\n",
    "\n",
    "4. __Ajuste de predicciones__: Las predicciones se ajustan considerando características adicionales del cliente, como recencia, frecuencia, y segmentación de negocio. Estas características se normalizan y se combinan para obtener un puntaje ajustado que mejora la recomendación final.\n",
    "\n",
    "5. __Evaluación de las recomendaciones__: Finalmente, se mide la precisión y el recall del modelo. La precisión representa la proporción de productos recomendados que fueron comprados, mientras que el recall mide la proporción de productos comprados que fueron recomendados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from surprise import Dataset, Reader, SVD, KNNBasic, accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Convertir july_df_filtrado a formato largo para Surprise\n",
    "def preparar_datos(df, rating_col='ITEMS_PHYS_CASES'):\n",
    "    reader = Reader(rating_scale=(1, df[rating_col].max()))\n",
    "    data = Dataset.load_from_df(df[['ACCOUNT_ID', 'SKU_ID', rating_col]], reader)\n",
    "    return data\n",
    "\n",
    "# Preparamos los datos de julio\n",
    "data_july = preparar_datos(july_df_filtrado)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "trainset_july, testset_july = train_test_split(data_july, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las opciones de similitud para KNN (item-based y user-based)\n",
    "sim_options_item = {'name': 'cosine', 'user_based': False}\n",
    "sim_options_user = {'name': 'cosine', 'user_based': True}\n",
    "\n",
    "# Crear los modelos KNN\n",
    "modelo_knn_item = KNNBasic(sim_options=sim_options_item)\n",
    "modelo_knn_user = KNNBasic(sim_options=sim_options_user)\n",
    "\n",
    "# Entrenar los modelos con los datos de julio\n",
    "modelo_knn_item.fit(trainset_july)\n",
    "modelo_knn_user.fit(trainset_july)\n",
    "\n",
    "# Generar predicciones para KNN (Item-based)\n",
    "predicciones_knn_item_august = modelo_knn_item.test(testset_july)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recomendaciones:\n",
    "\n",
    "Para el cliente especificado (`cliente_id`), el código genera recomendaciones excluyendo los productos que ya compró en julio y selecciona los top N productos con las mejores predicciones.\n",
    "\n",
    "__Salida en Formato DataFrame__:\n",
    "\n",
    "El resultado se presenta como un DataFrame con las columnas `ACCOUNT_ID`, `SKU_ID`, `SCORE`, y `FECHA_DE_RECOMENDACION`, mostrando las recomendaciones para ese cliente en particular.\n",
    "\n",
    "__Visualización del DataFrame__:\n",
    "\n",
    "El código imprime el DataFrame que contiene las recomendaciones para el cliente.\n",
    "\n",
    "__Ejemplo de Salida Esperada__:\n",
    "\n",
    "| ACCOUNT_ID | SKU_ID | SCORE | FECHA_DE_RECOMENDACION |\n",
    "|------------|--------|-------|------------------------|\n",
    "| 12345      | 7890   | 0.85  | 2024-09-18             |\n",
    "| 12345      | 2345   | 0.78  | 2024-09-18             |\n",
    "| 12345      | 6789   | 0.65  | 2024-09-18             |\n",
    "| ...        | ...    | ...   | 2024-09-18             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ajustar_predicciones_knn(predicciones, df_caracteristicas, account_id, top_n=10):\n",
    "    pred_df = pd.DataFrame(predicciones, columns=['ACCOUNT_ID', 'SKU_ID', 'rating_real', 'rating_pred', 'details'])\n",
    "\n",
    "    # Filtrar solo las predicciones para el cliente especificado\n",
    "    pred_df = pred_df[pred_df['ACCOUNT_ID'] == account_id]\n",
    "    pred_df = pred_df.merge(df_caracteristicas, on=['ACCOUNT_ID', 'SKU_ID'], how='left')\n",
    "\n",
    "    # Normalizar algunas variables clave para su uso en el ajuste\n",
    "    pred_df['RECENCIA_NORMALIZADA'] = 1 / (1 + pred_df['RECENCIA'].clip(lower=0))\n",
    "    pred_df['FRECUENCIA_NORMALIZADA'] = pred_df['FRECUENCIA'] / pred_df['FRECUENCIA'].max()\n",
    "    pred_df['TOTAL_ITEMS_NORMALIZADA'] = pred_df['TOTAL_ITEMS'] / pred_df['TOTAL_ITEMS'].max()\n",
    "\n",
    "    # Ajustar las predicciones utilizando una combinación de variables importantes\n",
    "    pred_df['SCORE_AJUSTADO'] = pred_df['rating_pred'] * (\n",
    "        pred_df['RECENCIA_NORMALIZADA'] * 0.2 + \n",
    "        pred_df['FRECUENCIA_NORMALIZADA'] * 0.3 + \n",
    "        pred_df['TOTAL_ITEMS_NORMALIZADA'] * 0.2 +\n",
    "        pred_df['MA_3'] * 0.1 + \n",
    "        pred_df['LAG_1'] * 0.1 +\n",
    "        pred_df['BUSINESS_SEGMENT_MediumUsage'] * 0.05 + \n",
    "        pred_df['CHANNEL_Mayorista'] * 0.05\n",
    "    )\n",
    "\n",
    "    # Clip para asegurar que los puntajes ajustados no sean negativos\n",
    "    pred_df['SCORE_AJUSTADO'] = pred_df['SCORE_AJUSTADO'].clip(lower=0)\n",
    "\n",
    "    # Eliminar duplicados de SKU_ID para el mismo ACCOUNT_ID\n",
    "    pred_df = pred_df.drop_duplicates(subset=['ACCOUNT_ID', 'SKU_ID'])\n",
    "\n",
    "    # Ordenar por SCORE_AJUSTADO en orden descendente y seleccionar el top_n\n",
    "    pred_df = pred_df.sort_values(by='SCORE_AJUSTADO', ascending=False).head(top_n)\n",
    "\n",
    "    # Añadir columna de fecha de recomendación\n",
    "    pred_df['FECHA_DE_RECOMENDACION'] = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    return pred_df[['ACCOUNT_ID', 'SKU_ID', 'SCORE_AJUSTADO', 'FECHA_DE_RECOMENDACION']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso:\n",
    "cliente_id = 456111   # Input del cliente específico\n",
    "\n",
    "# Ajustar las predicciones de KNN\n",
    "predicciones_ajustadas_knn = ajustar_predicciones_knn(predicciones_knn_item_august, august_df_filtrado, cliente_id, top_n=10)\n",
    "\n",
    "# Mostrar las recomendaciones ajustadas con KNN\n",
    "print(predicciones_ajustadas_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_recomendaciones(predicciones_ajustadas, df_real, cliente_id):\n",
    "    productos_comprados = df_real[df_real['ACCOUNT_ID'] == cliente_id]['SKU_ID'].unique()\n",
    "    productos_recomendados = predicciones_ajustadas['SKU_ID'].unique()\n",
    "    aciertos = [sku for sku in productos_recomendados if sku in productos_comprados]\n",
    "\n",
    "    precision = len(aciertos) / len(productos_recomendados) if productos_recomendados.size > 0 else 0\n",
    "    recall = len(aciertos) / len(productos_comprados) if productos_comprados.size > 0 else 0\n",
    "    return precision, recall\n",
    "\n",
    "# Evaluar recomendaciones ajustadas\n",
    "precision_knn, recall_knn = evaluar_recomendaciones(predicciones_ajustadas_knn, august_df_filtrado, cliente_id)\n",
    "print(f\"Precisión del modelo KNN: {precision_knn:.2f}, Recall del modelo KNN: {recall_knn:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Evaluación del modelo KNN__: En este caso, la precisión del modelo ``KNN`` fue del ``100%``, lo que indica que todos los productos recomendados fueron comprados. Sin embargo, el recall fue del ``40%``, lo que sugiere que el modelo no capturó todas las posibles compras realizadas por el cliente, lo que podría indicar que el modelo está recomendando un conjunto limitado de productos.\n",
    "\n",
    "Estos resultados pueden mejorarse ajustando los hiperparámetros del modelo KNN o integrando más información sobre el comportamiento del cliente para capturar mejor la diversidad de productos comprados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Función para generar recomendaciones con ``SVD``__:\n",
    "\n",
    "* Se filtran los productos que el cliente no compró en el mes anterior.\n",
    "\n",
    "* El modelo ``SVD`` predice los productos más relevantes basándose en las interacciones previas.\n",
    "\n",
    "* Las recomendaciones se almacenan en un DataFrame con las columnas ``ACCOUNT_ID``, ``SKU_ID``, ``SCORE``, y ``FECHA_DE_RECOMENDACION``.\n",
    "\n",
    "2. __Evaluación de las recomendaciones con ``SVD``__:\n",
    "\n",
    "* Se evalúa el rendimiento de las recomendaciones midiendo la precisión y el recall.\n",
    "\n",
    "* La precisión refleja la proporción de productos recomendados que fueron comprados por el cliente.\n",
    "\n",
    "* El recall mide la proporción de productos comprados que fueron recomendados.\n",
    "\n",
    "3. __Generación de recomendaciones combinadas con ``KNN (Item-based y User-based)``__:\n",
    "\n",
    "* Se generan dos conjuntos de predicciones: uno utilizando la similitud entre items (productos similares) y otro utilizando la similitud entre usuarios (clientes con patrones de compra similares).\n",
    "\n",
    "* Las predicciones se ajustan utilizando características adicionales del cliente, como ``recencia``, ``frecuencia`` y ``volumen total`` de compras, lo que ayuda a refinar las recomendaciones.\n",
    "\n",
    "4. __Evaluación de las recomendaciones con ``KNN``__:\n",
    "\n",
    "* Al igual que en el caso de ``SVD``, se calculan las métricas de ``precisión`` y ``recall`` para evaluar el rendimiento del modelo ``KNN`` combinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para generar recomendaciones usando SVD\n",
    "def generar_recomendaciones_para_cliente(modelo, cliente_id, productos, df_julio, top_n=10):\n",
    "    productos_comprados_julio = df_julio[df_julio['ACCOUNT_ID'] == cliente_id]['SKU_ID'].unique()\n",
    "    productos_no_comprados = [producto for producto in productos if producto not in productos_comprados_julio]\n",
    "\n",
    "    predicciones = [modelo.predict(cliente_id, producto_id) for producto_id in productos_no_comprados]\n",
    "    predicciones.sort(key=lambda x: x.est, reverse=True)\n",
    "    recomendaciones = predicciones[:top_n]\n",
    "    \n",
    "    fecha_recomendacion = datetime.now().strftime('%Y-%m-%d')\n",
    "    output = pd.DataFrame({\n",
    "        'ACCOUNT_ID': [cliente_id] * top_n,\n",
    "        'SKU_ID': [rec.iid for rec in recomendaciones],\n",
    "        'SCORE': [rec.est for rec in recomendaciones],\n",
    "        'FECHA_DE_RECOMENDACION': [fecha_recomendacion] * top_n\n",
    "    })\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso\n",
    "cliente_id = 456111\n",
    "productos = august_df_filtrado['SKU_ID'].unique()\n",
    "df_recomendaciones_cliente_svd = generar_recomendaciones_para_cliente(modelo_svd, cliente_id, productos, july_df_filtrado, top_n=10)\n",
    "\n",
    "# Mostrar las recomendaciones iniciales generadas por SVD\n",
    "print(\"Recomendaciones iniciales:\")\n",
    "print(df_recomendaciones_cliente_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar las recomendaciones\n",
    "def evaluar_recomendaciones(predicciones_ajustadas, df_agosto, cliente_id):\n",
    "    # Obtener los productos comprados en agosto por el cliente\n",
    "    productos_comprados_agosto = df_agosto[df_agosto['ACCOUNT_ID'] == cliente_id]['SKU_ID'].unique()\n",
    "    \n",
    "    # Verificar si la columna SKU_ID existe y contiene datos\n",
    "    if 'SKU_ID' not in predicciones_ajustadas.columns or predicciones_ajustadas['SKU_ID'].isnull().all():\n",
    "        print(\"Error: No se encuentran recomendaciones válidas en las predicciones ajustadas.\")\n",
    "        return 0, 0\n",
    "\n",
    "    # Obtener los productos recomendados\n",
    "    productos_recomendados = predicciones_ajustadas['SKU_ID'].dropna().unique()\n",
    "    \n",
    "    # Verificar si hay productos recomendados y comprados\n",
    "    if len(productos_recomendados) == 0 or len(productos_comprados_agosto) == 0:\n",
    "        print(\"No hay productos recomendados o comprados para evaluar.\")\n",
    "        return 0, 0\n",
    "    \n",
    "    # Obtener los aciertos entre los productos recomendados y los comprados\n",
    "    aciertos = [sku for sku in productos_recomendados if sku in productos_comprados_agosto]\n",
    "\n",
    "    # Calcular precisión (productos recomendados comprados / productos recomendados)\n",
    "    precision = len(aciertos) / len(productos_recomendados) if len(productos_recomendados) > 0 else 0\n",
    "\n",
    "    # Calcular recall (productos recomendados comprados / productos comprados en agosto)\n",
    "    recall = len(aciertos) / len(productos_comprados_agosto) if len(productos_comprados_agosto) > 0 else 0\n",
    "\n",
    "    return precision, recall\n",
    "\n",
    "# Evaluar las recomendaciones para el cliente\n",
    "precision, recall = evaluar_recomendaciones(predicciones_ajustadas_knn, august_df_filtrado, cliente_id)\n",
    "print(f\"Precisión: {precision:.2f}, Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def generar_recomendaciones_combinadas_ajustadas(modelo_item, modelo_user, cliente_id, productos, df_julio, df_caracteristicas, top_n=10):\n",
    "    \"\"\"\n",
    "    Genera recomendaciones combinadas utilizando KNN (item-based y user-based),\n",
    "    ajustando con características adicionales como RFM, medios móviles, y comportamiento del producto.\n",
    "    \"\"\"\n",
    "    # Filtrar los productos comprados por el cliente en julio\n",
    "    productos_comprados_julio = df_julio[df_julio['ACCOUNT_ID'] == cliente_id]['SKU_ID'].unique()\n",
    "\n",
    "    # Generar recomendaciones con KNN (item-based)\n",
    "    predicciones_item = [modelo_item.predict(cliente_id, producto_id) for producto_id in productos_comprados_julio]\n",
    "\n",
    "    # Generar recomendaciones con KNN (user-based)\n",
    "    predicciones_user = [modelo_user.predict(cliente_id, producto_id) for producto_id in productos_comprados_julio]\n",
    "\n",
    "    # Combinar ambas listas de predicciones sin duplicar SKU\n",
    "    recomendaciones_combinadas = {rec.iid: rec.est for rec in predicciones_item}\n",
    "    for rec in predicciones_user:\n",
    "        if rec.iid not in recomendaciones_combinadas:\n",
    "            recomendaciones_combinadas[rec.iid] = rec.est\n",
    "\n",
    "    # Crear DataFrame con las recomendaciones\n",
    "    df_recomendaciones = pd.DataFrame(recomendaciones_combinadas.items(), columns=['SKU_ID', 'SCORE'])\n",
    "\n",
    "    # Incorporar las características del cliente desde df_caracteristicas\n",
    "    df_recomendaciones = df_recomendaciones.merge(df_caracteristicas[df_caracteristicas['ACCOUNT_ID'] == cliente_id], on='SKU_ID', how='left')\n",
    "\n",
    "    # Verificar si hay valores faltantes en las columnas de ajuste\n",
    "    columnas_ajuste = ['RECENCIA', 'FRECUENCIA', 'TOTAL_ITEMS', 'MA_3', 'LAG_1', 'BUSINESS_SEGMENT_MediumUsage', 'CHANNEL_Mayorista']\n",
    "    if df_recomendaciones[columnas_ajuste].isnull().any().any():\n",
    "        print(\"Existen valores nulos en las columnas de ajuste. Rellenando con valores predeterminados...\")\n",
    "        df_recomendaciones['RECENCIA'] = df_recomendaciones['RECENCIA'].fillna(0)\n",
    "        df_recomendaciones['FRECUENCIA'] = df_recomendaciones['FRECUENCIA'].fillna(1)\n",
    "        df_recomendaciones['TOTAL_ITEMS'] = df_recomendaciones['TOTAL_ITEMS'].fillna(1)\n",
    "        df_recomendaciones['MA_3'] = df_recomendaciones['MA_3'].fillna(0)\n",
    "        df_recomendaciones['LAG_1'] = df_recomendaciones['LAG_1'].fillna(0)\n",
    "        df_recomendaciones['BUSINESS_SEGMENT_MediumUsage'] = df_recomendaciones['BUSINESS_SEGMENT_MediumUsage'].fillna(0)\n",
    "        df_recomendaciones['CHANNEL_Mayorista'] = df_recomendaciones['CHANNEL_Mayorista'].fillna(0)\n",
    "\n",
    "    # Normalizar las variables clave para el ajuste\n",
    "    df_recomendaciones['RECENCIA_NORMALIZADA'] = 1 / (1 + df_recomendaciones['RECENCIA'].clip(lower=0))\n",
    "    df_recomendaciones['FRECUENCIA_NORMALIZADA'] = df_recomendaciones['FRECUENCIA'] / df_recomendaciones['FRECUENCIA'].max()\n",
    "    df_recomendaciones['TOTAL_ITEMS_NORMALIZADO'] = df_recomendaciones['TOTAL_ITEMS'] / df_recomendaciones['TOTAL_ITEMS'].max()\n",
    "\n",
    "    # Ajustar el SCORE utilizando una combinación de las variables importantes\n",
    "    df_recomendaciones['SCORE_AJUSTADO'] = df_recomendaciones['SCORE'] * (\n",
    "        df_recomendaciones['RECENCIA_NORMALIZADA'] * 0.2 + \n",
    "        df_recomendaciones['FRECUENCIA_NORMALIZADA'] * 0.3 + \n",
    "        df_recomendaciones['TOTAL_ITEMS_NORMALIZADO'] * 0.2 +\n",
    "        df_recomendaciones['MA_3'] * 0.1 + \n",
    "        df_recomendaciones['LAG_1'] * 0.1 +\n",
    "        df_recomendaciones['BUSINESS_SEGMENT_MediumUsage'] * 0.05 + \n",
    "        df_recomendaciones['CHANNEL_Mayorista'] * 0.05\n",
    "    )\n",
    "\n",
    "    # Clip para asegurar que los puntajes ajustados no sean negativos\n",
    "    df_recomendaciones['SCORE_AJUSTADO'] = df_recomendaciones['SCORE_AJUSTADO'].clip(lower=0)\n",
    "\n",
    "    # Eliminar duplicados de SKU_ID\n",
    "    df_recomendaciones = df_recomendaciones.drop_duplicates(subset=['SKU_ID'])\n",
    "\n",
    "    # Completar ACCOUNT_ID faltante\n",
    "    df_recomendaciones['ACCOUNT_ID'] = cliente_id\n",
    "\n",
    "    # Ordenar por SCORE_AJUSTADO en orden descendente y seleccionar el top_n\n",
    "    df_recomendaciones = df_recomendaciones.sort_values(by='SCORE_AJUSTADO', ascending=False).head(top_n)\n",
    "\n",
    "    # Resetear el índice para el DataFrame final\n",
    "    df_recomendaciones = df_recomendaciones.reset_index(drop=True)\n",
    "\n",
    "    return df_recomendaciones[['ACCOUNT_ID', 'SKU_ID', 'SCORE_AJUSTADO']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso:\n",
    "cliente_id = 456111  # ID del cliente específico\n",
    "productos = july_df['SKU_ID'].unique()  # Lista de productos del mes de junio para la base de recomendaciones\n",
    "\n",
    "# Generar el top N de recomendaciones combinadas y ajustadas\n",
    "df_recomendaciones_cliente = generar_recomendaciones_combinadas_ajustadas(\n",
    "    modelo_knn_item, modelo_knn_user, cliente_id, productos, july_df_filtrado, august_df_filtrado, top_n=10)\n",
    "\n",
    "# Mostrar las recomendaciones para el cliente\n",
    "print(df_recomendaciones_cliente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_recomendaciones(df_recomendaciones, df_agosto, cliente_id):\n",
    "    \"\"\"\n",
    "    Evalúa las recomendaciones revisando si el cliente compró los productos en agosto.\n",
    "    Calcula precisión y recall.\n",
    "    \"\"\"\n",
    "    # Obtener los productos comprados por el cliente en agosto\n",
    "    productos_comprados_agosto = df_agosto[df_agosto['ACCOUNT_ID'] == cliente_id]['SKU_ID'].unique()\n",
    "\n",
    "    # Obtener los productos recomendados\n",
    "    productos_recomendados = df_recomendaciones['SKU_ID'].unique()\n",
    "\n",
    "    # Verificar si las recomendaciones están entre los productos comprados\n",
    "    aciertos = [sku for sku in productos_recomendados if sku in productos_comprados_agosto]\n",
    "\n",
    "    # Calcular precisión (proporción de productos recomendados que fueron comprados)\n",
    "    precision = len(aciertos) / len(productos_recomendados) if len(productos_recomendados) > 0 else 0\n",
    "\n",
    "    # Calcular recall (proporción de productos comprados que fueron recomendados)\n",
    "    recall = len(aciertos) / len(productos_comprados_agosto) if len(productos_comprados_agosto) > 0 else 0\n",
    "\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall = evaluar_recomendaciones(df_recomendaciones_cliente, august_df_filtrado, cliente_id)\n",
    "print(f\"Precisión: {precision:.2f}, Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este enfoque híbrido logra una mejor personalización al combinar dos métodos complementarios. ``KNN es útil para encontrar productos o usuarios similares``, mientras que ``SVD captura patrones latentes en los datos``. La normalización y el ajuste basado en características del cliente ayudan a optimizar las recomendaciones, obteniendo una ``precisión`` de ``0.90`` y un ``recall`` de ``0.6``, lo que indica que el modelo logra recomendar con acierto una gran parte de los productos que el cliente finalmente compra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __BONUS TRACK__: Clustering de características RFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correlación para RFM\n",
    "plt.figure(figsize=(8, 6))\n",
    "correlation_matrix_rfm = rfm_df.corr()\n",
    "sns.heatmap(correlation_matrix_rfm, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title('Matriz de Correlación entre Variables RFM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# Definir una lista de posibles valores de k para el método del codo\n",
    "k_values = range(2, 8)\n",
    "\n",
    "# Inicializar listas para almacenar las métricas\n",
    "inertia_values = []\n",
    "# Realizar clustering con diferentes valores de k y calcular las métricas\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(rfm_df)\n",
    "    labels = kmeans.labels_\n",
    "    # Calcular la inercia\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "\n",
    "# Graficar el método del codo utilizando la inercia\n",
    "plt.plot(k_values, inertia_values, 'bo-')\n",
    "plt.xlabel('Número de clusters (k)')\n",
    "plt.ylabel('Inercia')\n",
    "plt.title('Método del Codo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar K-Means con 4 clusters en el conjunto completo de datos\n",
    "kmeans_4 = KMeans(n_clusters=4, random_state=42, n_init=10, max_iter=300)\n",
    "rfm_df['Cluster'] = kmeans_4.fit_predict(rfm_df)\n",
    "\n",
    "# Describir los clusters resultantes\n",
    "cluster_summary = rfm_df.groupby('Cluster').agg({\n",
    "    'RECENCIA': ['mean', 'median'],\n",
    "    'FRECUENCIA': ['mean', 'median'],\n",
    "    'TOTAL_ITEMS': ['mean', 'median'],\n",
    "    'ACCOUNT_ID': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "# Mostrar el resumen de los clusters\n",
    "cluster_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Asignar el valor óptimo de k (elegido por el método del codo)\n",
    "optimal_k = 4\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "kmeans.fit(rfm_df)\n",
    "\n",
    "# Obtener las etiquetas de los clusters\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Añadir las etiquetas de los clusters al DataFrame\n",
    "rfm_df['clusters'] = labels\n",
    "\n",
    "# Reducir la dimensionalidad a 2 componentes principales con PCA para graficar\n",
    "pca = PCA(n_components=2)\n",
    "rfm_pca = pca.fit_transform(rfm_df.drop(columns=['clusters']))\n",
    "\n",
    "# Graficar los clusters en el espacio reducido por PCA\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(rfm_pca[:, 0], rfm_pca[:, 1], c=labels, cmap='viridis')\n",
    "plt.xlabel('Componente Principal 1')\n",
    "plt.ylabel('Componente Principal 2')\n",
    "plt.title(f'Clusters K-means (k={optimal_k})')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "# Suponiendo que X es tu conjunto de datos y labels son las etiquetas del clustering\n",
    "silhouette_scores = silhouette_score(rfm_df, labels)\n",
    "davies_bouldin_scores = davies_bouldin_score(rfm_df, labels)\n",
    "\n",
    "print(f'Silhouette Score: {silhouette_scores:.4f}')\n",
    "print(f'Davies-Bouldin Score: {davies_bouldin_scores:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-fuzzy\n",
    "import skfuzzy as fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir una lista de posibles valores de m\n",
    "m_values = np.arange(1.2, 3.8, 0.1)\n",
    "\n",
    "# Inicializar listas para almacenar las métricas\n",
    "silhouette_scores_fuzzy = []\n",
    "davies_bouldin_scores_fuzzy = []\n",
    "\n",
    "# Realizar clustering fuzzy con diferentes valores de m y calcular las métricas\n",
    "for m in m_values:\n",
    "    cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(rfm_df.T, 4, m, error=0.005, maxiter=1000)\n",
    "    labels = np.argmax(u, axis=0)\n",
    "\n",
    "    # Calcular la puntuación de silueta\n",
    "    silhouette_scores_fuzzy.append(silhouette_score(rfm_df, labels))\n",
    "\n",
    "    # Calcular el índice de Davies-Bouldin\n",
    "    dunn_index = davies_bouldin_score(rfm_df, labels)\n",
    "    davies_bouldin_scores_fuzzy.append(dunn_index)\n",
    "\n",
    "# Encontrar el valor óptimo de m basado en la puntuación de silueta\n",
    "optimal_index = np.argmax(silhouette_scores_fuzzy)\n",
    "optimal_m = m_values[optimal_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar Fuzzy C-Means con el valor óptimo de m\n",
    "n_clusters = 4\n",
    "cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(rfm_df.T, n_clusters, optimal_m, error=0.005, maxiter=1000)\n",
    "\n",
    "# Obtener la pertenencia de cada punto al cluster más cercano\n",
    "cluster_membership = np.argmax(u, axis=0)\n",
    "\n",
    "# Añadir la asignación de clusters al DataFrame original\n",
    "rfm_df['Fuzzy_Cluster'] = cluster_membership\n",
    "\n",
    "# Resumir los clusters resultantes\n",
    "fuzzy_cluster_summary = rfm_df.groupby('Fuzzy_Cluster').agg({\n",
    "    'RECENCIA': ['mean', 'median'],\n",
    "    'FRECUENCIA': ['mean', 'median'],\n",
    "    'TOTAL_ITEMS': ['mean', 'median'],\n",
    "    'ACCOUNT_ID': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "# Imprimir el valor óptimo de m y el resumen de los clusters\n",
    "print(f\"Valor óptimo de m: {optimal_m}\")\n",
    "print()\n",
    "print(fuzzy_cluster_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar los resultados en 2D\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Resultados del Clustering K-means\n",
    "plt.subplot(121)\n",
    "plt.scatter(rfm_pca[:, 0], rfm_pca[:, 1], c=labels, cmap='viridis')\n",
    "plt.title('Clustering K-means')\n",
    "\n",
    "# Resultados del Clustering Fuzzy C-means\n",
    "plt.subplot(122)\n",
    "plt.scatter(rfm_pca[:, 0], rfm_pca[:, 1], c=cluster_membership, cmap='viridis')\n",
    "plt.title('Clustering Fuzzy C-means')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar la puntuación de silueta y el índice de Dunn\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(m_values, silhouette_scores_fuzzy, 'bo-', label='Puntuación de silueta')\n",
    "plt.plot(m_values, davies_bouldin_scores_fuzzy, 'ro-', label='Índice de Dunn')\n",
    "plt.xlabel('Valor de m')\n",
    "plt.ylabel('Métrica')\n",
    "plt.title('Evaluación de Clustering Fuzzy C-means')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer los valores numéricos de las listas en Clustering Fuzzy\n",
    "silhouette_scores_fuzzy_value = silhouette_scores_fuzzy[0] if isinstance(silhouette_scores_fuzzy, list) else silhouette_scores_fuzzy\n",
    "davies_bouldin_scores_fuzzy_value = davies_bouldin_scores_fuzzy[0] if isinstance(davies_bouldin_scores_fuzzy, list) else davies_bouldin_scores_fuzzy\n",
    "\n",
    "# Crear un DataFrame para comparar las métricas\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Métrica': ['Silhouette Score', 'Davies-Bouldin Score'],\n",
    "    'Clustering K-means': [silhouette_scores, davies_bouldin_scores],\n",
    "    'Clustering Fuzzy': [silhouette_scores_fuzzy_value, davies_bouldin_scores_fuzzy_value]\n",
    "})\n",
    "\n",
    "# Mostrar el DataFrame comparativo\n",
    "print(comparison_df)\n",
    "\n",
    "# Visualización de la comparación\n",
    "comparison_df.set_index('Métrica').plot(kind='bar', figsize=(10, 6), color=['skyblue', 'lightgreen'])\n",
    "plt.title('Comparación de Métricas: Clustering K-means vs Clustering Fuzzy')\n",
    "plt.ylabel('Valor de la Métrica')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Resumen de las clasificaciones__:\n",
    "\n",
    "* `Clúster 0`: Compradores Ocasionales Activos – compran regularmente, pero en cantidades moderadas.\n",
    "\n",
    "* `Clúster 1`: Compradores Fieles y Activos – clientes que compran grandes volúmenes y con frecuencia, los mejores clientes.\n",
    "\n",
    "* `Clúster 2`: Compradores Frecuentes y Moderados – compran de manera constante, pero en cantidades moderadas.\n",
    "\n",
    "* `Clúster 3`: Compradores de Volumen Pequeño – clientes que hacen compras pequeñas y frecuentes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
